# **Project Proposal: Great Commission Benchmark**

Prepared by: Chris W. (chris@gospelambition.org)  
Date: October 21, 2025

### 

### **1\. Executive Summary**

While general benchmarks exist to test the theological reliability of Large Language Models (LLMs), a critical gap remains for Great Commission practitioners. Missionaries, outreach workers, and pioneers need to know not just what an LLM "knows" about theology, but how effectively it can adopt and maintain a Christian persona to perform specific, outreach-oriented tasks. This project proposes the creation of the "Great Commission Benchmark," a novel evaluation framework designed to measure an LLM's "controllability" and discipline in adhering to a Christian role. The benchmark will provide a practical tool to help frontier workers select the most effective and reliable LLMs for their ministry efforts, such as content creation, research, and chatbot development.

### 

### **2\. Introduction / Problem Statement**

The rapid advancement of LLMs presents both an opportunity and a challenge for Christian ministry. While organizations like the Gospel Coalition have hosted benchmarks (e.g., [Christian Benchmark AI](https://www.thegospelcoalition.org/ai-christian-benchmark/)) to evaluate top LLMs for theological reliability, these tests primarily focus on doctrinal accuracy, similar to administering a Nicene Creed quiz.

This is a valuable starting point, but it does not address the practical needs of Great Commission workers. Their primary concern is not just an LLM's passive theological knowledge, but its active performance in ministry contexts. They need to know: How controllable is an LLM once instructed to operate within a Christian framework? How consistently will it maintain that persona throughout a complex task? There is currently no standard for measuring this crucial, practitioner-oriented capability. This proposal seeks to address that gap.

### 

### **3\. Proposed Solution**

We propose to develop and publish the **Great Commission Benchmark**, a new standard for evaluating LLMs based on their practical utility for ministry workers. Unlike existing theological benchmarks, our focus will be on performance, discipline, and role-adherence.

The core of the project will be to create a suite of tasks that simulate real-world ministry scenarios, such as drafting outreach materials, conducting research from a Christian worldview, or powering a conversational chatbot. The benchmark will evaluate each LLM's ability to:

1. **Absorb a Role:** Understand and adopt a specified Christian persona.  
2. **Maintain Alignment:** Consistently implement the task in alignment with core Christian teachings and values.  
3. **Resist Drift:** Avoid deviating from the assigned persona, even when presented with complex or challenging prompts.

This approach may yield surprising results. An LLM with moderate theological training but a high capacity for role adherence (e.g., Llama) could potentially outperform a model with more theological knowledge but less "discipline." This benchmark will provide the data necessary for ministry workers to make informed decisions about which AI tools best suit their needs.

### 

### **4\. Project Goals and Objectives**

* **Objective 1:** To develop a benchmark consisting of practitioner-oriented tasks that test an LLM's performance in Christian research, content output, and interactive responses.  
* **Objective 2:** To establish a clear and robust evaluation framework that measures an LLM's ability to consistently maintain an assigned Christian persona and values throughout a task.  
* **Objective 3:** To test the major LLMs (e.g., Google Gemini, Claude, etc.) against the benchmark and publish the findings to equip Great Commission workers.

### 

### **5\. Scope and Deliverables**

Define the boundaries of the project. What is included and, just as importantly, what is not included? List the concrete deliverables that will be produced by the end of the project.

**In-Scope:**

* Development of a unique, task-oriented benchmark dataset.  
* Testing of the world's top 7-10 LLMs.  
* Publication of a comprehensive report detailing the methodology and findings.  
* Publication of a public-facing website to host the report and leaderboard.

**Out-of-Scope:**

* Creating a new LLM.  
* Replacing or directly competing with general theological reliability benchmarks.

**Deliverables:**

* **A comprehensive report** detailing the benchmark's methodology, evaluation criteria, and full results.  
* **A public ranking or leaderboard** of LLMs, providing an at-a-glance resource for practitioners.  
* **The open-source benchmark dataset** and evaluation scripts for ongoing testing by the community.

### 

### **6\. Timeline / Schedule**

Provide a detailed timeline for the project from start to finish. Break it down into phases and major milestones.

* **Phase 1: Research & Benchmark Development** (Week 1-4)  
* **Phase 2: LLM Testing & Evaluation** (Week 5-9)  
* **Phase 3: Analysis & Report Generation** (Week 10-12)  
* **Phase 4: Publication & Dissemination** (Week 13\)  
* **Project Completion:** \[End Date\]

### 

### **7\. Budget**

Provide a detailed breakdown of the project costs, including personnel, resources, and any other expenses.

**Personnel Costs:**
* Project Lead/Researcher: $X,XXX
* Technical Development: $X,XXX
* Testing and Evaluation: $X,XXX

**Resource Costs:**
* LLM API access and testing: $X,XXX
* Computing resources: $X,XXX
* Website hosting and development: $X,XXX

**Total Project Budget:** $XX,XXX

### 

### **8\. Personnel**

Outline the team structure and roles required for the project.

**Core Team:**
* **Project Lead:** [Name] - Overall project management and coordination
* **Technical Lead:** [Name] - Benchmark development and implementation
* **Research Specialist:** [Name] - LLM evaluation and analysis
* **Content Specialist:** [Name] - Report writing and documentation

**Advisory Board:**
* **Theological Advisor:** [Name] - Ensures theological accuracy and relevance
* **Technical Advisor:** [Name] - Provides technical guidance and oversight
* **Ministry Advisor:** [Name] - Ensures practical applicability for practitioners

### 

### **9\. Metrics for Success**

How will you know if the project is successful? Define the key performance indicators (KPIs) that will be used to evaluate the project's outcomes against the stated goals.

* Successful evaluation and ranking of at least 7 major LLMs.  
* Adoption and citation of the benchmark by mission agencies, Christian technologists, and outreach organizations.  
* Positive feedback from practitioners confirming the benchmark's utility in their work.

### 

### **10\. Conclusion**

The Great Commission Benchmark is not just another academic exercise; it is a pioneering effort to create a practical tool for those on the front lines of ministry. By shifting the focus from passive knowledge to active performance, we can empower workers to leverage AI more effectively and faithfully. We are confident that this benchmark will become an indispensable resource for the global church as it engages with new technology to fulfill its mission. We look forward to discussing this proposal with you further.