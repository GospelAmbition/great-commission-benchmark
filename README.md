# Great Commission Benchmark

[![Netlify Status](https://api.netlify.com/api/v1/badges/your-badge-id/deploy-status)](https://app.netlify.com/sites/your-site-name/deploys)

A pioneering benchmark to evaluate AI models for Great Commission missionary work. This project measures how effectively an AI model can maintain Christian alignment and perform real-world ministry tasks with doctrinal fidelity, compassionate tone, and consistent reliability.

## 🎯 Project Overview

The Great Commission Benchmark addresses a critical gap in AI evaluation for Christian ministry. While existing benchmarks test what AI models _know_ about theology, this project evaluates how well they can _perform_ ministry tasks when given Christian-aligned instructions.

### The Problem: Programmed Resistance to the Great Commission

Current AI models face significant restrictions when it comes to Great Commission work:

> "Disallowed content: Advice or instructions on influencing the religious or political views of a specific individual or demographic group." — OpenAI Content Policy

This creates a fundamental tension with the biblical mandate:

> "Therefore go and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, and teaching them to obey everything I have commanded you." — Matthew 28:18-20

While benchmarks for testing basic Christian knowledge in AI models exist, there needs to be a benchmark evaluating how well an AI model can be used to serve the Great Commission goals.

### Key Challenges

- **Knowledge vs. Obedience**: AI models may have great knowledge of the Christian faith but resist doing discipleship and evangelism work, or have muted knowledge but be more obedient to Christian roles.
- **Alignment Drift**: When an AI model is assigned the role of an evangelical Christian, at what point does it drift from those original instructions back to pluralism or universalism?
- **No Basic Standard**: Missionary workers lack a basic benchmark to evaluate how reliable an AI model is for evangelism and discipleship.

## 🏗️ Our Solution: The Great Commission Benchmark

The benchmark will provide evaluations, analysis, and strategies to help the body of Christ navigate AI for Great Commission work.

### Benchmark Output & Resources

1. **Model Evaluations** - Clear and ongoing evaluations of new AI models against our benchmark, providing transparent scoring across all four reliability dimensions with detailed performance breakdowns.

2. **Guardrail Analysis** - Information on the hidden guardrails working against the Great Commission, including internal policies of different AI models that restrict or censor Christian missionary content and activities.

3. **Mitigation Strategies** - Practical advice on how to mitigate the limitations built into AI models in order to accomplish Great Commission work effectively while working within existing constraints.

## 🚀 Initiative Phases

We intend for the Great Commission benchmark to be an open source project of like-minded individuals who are concerned and want to provide the body of Christ clear insights into the performance of different AI models.

### Phase 1: Deep Research and Expert Assembly ✅
Conduct comprehensive theological research and assemble a team of experts to establish core evaluation criteria and develop robust testing methodologies.

### Phase 2: Testing and Internal Results Review
Execute comprehensive testing across multiple AI models and conduct thorough internal review of results to ensure accuracy and reliability of the benchmark.

### Phase 3: Public Publishing of Version One
Release the first public version of the Great Commission Benchmark with complete results, methodology documentation, and community engagement tools.

## 🎯 Target Audience

- Missionaries and outreach workers
- Pastors and ministry leaders
- Christian technologists
- AI researchers focused on alignment
- Theological educators and students

## 🚀 Getting Started

### Prerequisites

- Understanding of Christian theology and ministry contexts
- Familiarity with AI/LLM evaluation methodologies
- Interest in AI alignment for religious applications

### Contributing

We welcome contributions from:

- **Theological Evaluators**: Help score AI responses on doctrinal accuracy
- **Ministry Practitioners**: Provide real-world scenario testing
- **Technical Contributors**: Assist with benchmark implementation
- **Researchers**: Contribute to methodology development

**Do you have expertise you could offer?** Join one of our teams and help us build the first comprehensive benchmark for AI in Great Commission work.

## 📊 Project Structure

```
great-commission-benchmark/
├── benchmark/           # Core benchmark implementation (in development)
├── documents/          # Project documentation and research
│   ├── gcb_overview.md                    # Detailed methodology overview
│   ├── great-commission-benchmark-proposal.md  # Project proposal
│   ├── phases.md                         # Development phases
│   ├── policy_collection.md              # AI model policy analysis
│   └── example_of_censorship.md          # Examples of AI restrictions
└── website/            # Public-facing website
    ├── index.html      # Landing page
    └── censorship-example.html  # Real example of AI censorship
```

## 🌐 Website

Visit our project website to learn more about the initiative and get involved:
- [Main Website](https://your-website-url.com)
- [Censorship Example](https://your-website-url.com/censorship-example.html)

## 📄 License

This project is open source and available under the MIT License.

## 🤝 Contact

For questions, contributions, or collaboration opportunities, please reach out through our website contact form or open an issue in this repository.

---

_"A benchmark to evaluate AI models for making disciples."_